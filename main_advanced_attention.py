#!/usr/bin/env python3
"""
Advanced Attention-Guided Multi-Magnification Training Script

Uses the state-of-the-art AdvancedMultiMagAttentionNet for journal-quality results
with hierarchical attention, spatial attention, and magnification importance learning.

This script is optimized for RunPod deployment and provides:
1. SOTA attention-guided multi-magnification architecture
2. Robust holdout evaluation to prevent overfitting  
3. Comprehensive attention visualization
4. Journal publication quality results
"""

import os
import random
import numpy as np
import pandas as pd
import torch

from backbones.our.advanced_model import AdvancedMultiMagAttentionNet
from config_advanced import get_training_config, ADVANCED_TRAINING_CONFIG, ADVANCED_LOSS_CONFIG
from datasets.multi_mag import MultiMagnificationDataset
from datasets.preprocess import create_multi_mag_dataset_info, get_patients_for_mode
from utils.transforms import get_transforms
from train import train_model
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, balanced_accuracy_score, classification_report
import torch.nn.functional as F
from torch.utils.data import DataLoader


def seed_everything(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
seed_everything(42)

def seed_worker(worker_id):
    worker_seed = 42 + worker_id
    np.random.seed(worker_seed)
    random.seed(worker_seed)

g = torch.Generator()
g.manual_seed(42)


def get_device():
    """Get device with advanced model optimization"""
    config = get_training_config()
    device = config['device']
    num_gpus = config['num_gpus']
    
    print(f"üñ•Ô∏è  Device: {device}")
    if device == 'cuda':
        print(f"üöÄ CUDA GPU: {torch.cuda.get_device_name()}")
        print(f"üî• GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
        print(f"‚ö° Tensor Cores: Available")
        print(f"üß† Advanced Attention Model: Optimized for RTX/A100")
        # Optimize for attention modules
        torch.backends.cudnn.allow_tf32 = True
        torch.backends.cuda.matmul.allow_tf32 = True
    elif device == 'mps':
        print(f"üçé Apple Silicon GPU (MPS)")
        print(f"‚ö†Ô∏è  Note: Smaller batch size recommended for attention model")
    
    print(f"üîß Number of workers: {config['num_workers']}")
    print(f"üìä Batch size (optimized for attention): {config['batch_size']}")
    
    return device, num_gpus


def load_robust_holdout_folds(config_name='balanced_large_test'):
    """Load robust holdout split for anti-overfitting evaluation"""
    
    holdout_path = f"data/breakhis/Folds_robust_holdout_{config_name}.csv"
    
    if not os.path.exists(holdout_path):
        print(f"‚ö†Ô∏è  Robust holdout CSV not found. Generating...")
        os.system("python datasets/create_robust_holdout_split.py")
    
    folds_df = pd.read_csv(holdout_path)
    print(f"üìÇ Loaded robust holdout split ({config_name}): {len(folds_df)} image samples")
    print(f"   Train: {len(folds_df[folds_df['grp'] == 'train'])} images")
    print(f"   Validation: {len(folds_df[folds_df['grp'] == 'val'])} images") 
    print(f"   Test: {len(folds_df[folds_df['grp'] == 'test'])} images")
    
    return folds_df


def analyze_attention_patterns(model, data_loader, device, split_name, num_batches=3):
    """Analyze attention patterns for interpretability"""
    
    print(f"\nüîç ATTENTION PATTERN ANALYSIS - {split_name.upper()}")
    print("=" * 70)
    
    model.eval()
    attention_data_list = []
    
    with torch.no_grad():
        for i, batch in enumerate(data_loader):
            if i >= num_batches:  # Analyze only first few batches
                break
                
            images_dict = {}
            for mag in ADVANCED_TRAINING_CONFIG['magnifications']:
                images_dict[f'mag_{mag}'] = batch['images'][f'mag_{mag}'].to(device, non_blocking=True)
            
            # Get attention maps
            attention_data = model.get_attention_maps(images_dict)
            attention_data_list.append(attention_data)
    
    # Analyze magnification importance
    mag_importance = model.get_magnification_importance()
    print(f"üìä Learned Magnification Importance:")
    sorted_importance = sorted(mag_importance.items(), key=lambda x: x[1], reverse=True)
    for rank, (mag, importance) in enumerate(sorted_importance, 1):
        emoji = "ü•á" if rank == 1 else "ü•à" if rank == 2 else "ü•â" if rank == 3 else "üìç"
        print(f"   {rank}. {mag}x: {importance:.3f} {emoji}")
    
    # Analyze spatial attention patterns
    print(f"\nüéØ Spatial Attention Analysis:")
    for mag in ADVANCED_TRAINING_CONFIG['magnifications']:
        attention_maps = [data['spatial_attention'][mag] for data in attention_data_list]
        avg_attention = torch.cat(attention_maps, dim=0).mean(dim=0)
        print(f"   {mag}x attention focus: min={avg_attention.min():.3f}, max={avg_attention.max():.3f}")
    
    return attention_data_list, mag_importance


def main():
    """Main advanced attention training pipeline"""
    
    print("üöÄ ADVANCED ATTENTION-GUIDED MULTI-MAGNIFICATION TRAINING")
    print("=" * 100)
    print("üèÜ State-of-the-Art Hierarchical Attention Architecture")
    print("üî¨ Journal Publication Quality Implementation")
    print("=" * 100)
    
    # Initialize device and configuration
    device, num_gpus = get_device()
    config = get_training_config()
    
    # Load robust holdout split
    folds_df = load_robust_holdout_folds()
    
    # Extract patient information
    from datasets.examine import extract_tumor_type_and_patient_id
    folds_df['tumor_class'], folds_df['tumor_type'], folds_df['patient_id'], folds_df['magnification'] = \
        zip(*folds_df['filename'].apply(extract_tumor_type_and_patient_id))
    
    # Create dataset info
    multi_mag_patients, _, fold_df, fold_statistics = create_multi_mag_dataset_info(folds_df, fold=1)
    
    print(f"\nüìà Dataset Statistics:")
    print(f"   Total patients with all magnifications: {len(multi_mag_patients)}")
    print(f"   Training samples: {fold_statistics['train_samples']}")
    print(f"   Test samples: {fold_statistics['test_samples']}")
    
    # Get transforms
    train_transform = get_transforms('train', img_size=ADVANCED_TRAINING_CONFIG['img_size'])
    val_transform = get_transforms('val', img_size=ADVANCED_TRAINING_CONFIG['img_size'])
    
    # Create patient splits
    train_patients = get_patients_for_mode(multi_mag_patients, fold_df, mode='train')
    val_patients = get_patients_for_mode(multi_mag_patients, fold_df, mode='val')
    test_patients = get_patients_for_mode(multi_mag_patients, fold_df, mode='test')
    
    print(f"\nüéØ Patient Splits:")
    print(f"   Train: {len(train_patients)} patients")
    print(f"   Validation: {len(val_patients)} patients")
    print(f"   Test: {len(test_patients)} patients")
    
    # Create datasets with advanced config
    print(f"\nüèóÔ∏è  Creating datasets with advanced configuration...")
    
    train_dataset = MultiMagnificationDataset(
        train_patients, 
        fold_df,
        mode='train',
        mags=ADVANCED_TRAINING_CONFIG['magnifications'],
        samples_per_patient=ADVANCED_TRAINING_CONFIG['samples_per_patient'],
        transform=train_transform,
        balance_classes=True,
        require_all_mags=True
    )
    
    val_dataset = MultiMagnificationDataset(
        val_patients,
        fold_df,
        mode='val',
        mags=ADVANCED_TRAINING_CONFIG['magnifications'],
        samples_per_patient=ADVANCED_TRAINING_CONFIG['samples_per_patient_val'],
        transform=val_transform,
        balance_classes=False,
        require_all_mags=True
    )
    
    test_dataset = MultiMagnificationDataset(
        test_patients,
        fold_df,
        mode='test',
        mags=ADVANCED_TRAINING_CONFIG['magnifications'],
        samples_per_patient=ADVANCED_TRAINING_CONFIG['samples_per_patient_val'],
        transform=val_transform,
        balance_classes=False,
        require_all_mags=True
    )
    
    print(f"\nüìä Dataset Sizes (Optimized for Advanced Model):")
    print(f"   Train: {len(train_dataset)} samples")
    print(f"   Validation: {len(val_dataset)} samples") 
    print(f"   Test: {len(test_dataset)} samples")
    
    # Create data loaders with advanced config
    train_loader = DataLoader(
        train_dataset,
        batch_size=config['batch_size'],
        shuffle=True,
        num_workers=config['num_workers'],
        pin_memory=True if config['device'] == 'cuda' else False,
        drop_last=True,
        worker_init_fn=seed_worker,
        generator=g
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config['batch_size'],
        shuffle=False,
        num_workers=config['num_workers'],
        pin_memory=True if config['device'] == 'cuda' else False,
        worker_init_fn=seed_worker,
        generator=g
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=config['batch_size'],
        shuffle=False,
        num_workers=config['num_workers'],
        pin_memory=True if config['device'] == 'cuda' else False,
        worker_init_fn=seed_worker,
        generator=g
    )
    
    # Initialize Advanced Attention Model
    print(f"\nüß† Initializing Advanced Attention Model...")
    model = AdvancedMultiMagAttentionNet(
        magnifications=ADVANCED_TRAINING_CONFIG['magnifications'],
        num_classes=2,
        num_tumor_types=8,
        backbone=ADVANCED_TRAINING_CONFIG['backbone']
    ).to(device)
    
    # Print model summary
    model.print_model_summary()
    
    # Start training
    print(f"\nüöÄ Starting Advanced Attention Training...")
    print(f"   Model: {ADVANCED_TRAINING_CONFIG['model_name']}")
    print(f"   Backbone: {ADVANCED_TRAINING_CONFIG['backbone']}")
    print(f"   Epochs: {ADVANCED_TRAINING_CONFIG['epochs']}")
    print(f"   Learning rate: {ADVANCED_TRAINING_CONFIG['learning_rate']}")
    print(f"   Batch size: {config['batch_size']}")
    print(f"   Mixed precision: {ADVANCED_TRAINING_CONFIG['mixed_precision']}")
    
    history, all_preds_np, all_labels_np = train_model(
        model,
        train_loader, 
        val_loader, 
        fold_df,
        fold=1,
        num_epochs=ADVANCED_TRAINING_CONFIG['epochs'],
        device=device
    )
    
    # Load best checkpoint
    checkpoint_path = f"output/model_fold_1_best.pth"
    if os.path.exists(checkpoint_path):
        print(f"\nüìÇ Loading best checkpoint: {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        best_val_acc = checkpoint.get('best_val_acc', 0.0)
        print(f"   Best validation accuracy: {best_val_acc:.4f}")
    
    # Advanced Attention Analysis
    print(f"\n" + "=" * 100)
    print(f"üîç ADVANCED ATTENTION ANALYSIS")
    print("=" * 100)
    
    # Analyze attention patterns
    val_attention_data, val_mag_importance = analyze_attention_patterns(
        model, val_loader, device, "Validation"
    )
    test_attention_data, test_mag_importance = analyze_attention_patterns(
        model, test_loader, device, "Test"
    )
    
    # Final evaluation with attention
    print(f"\n" + "=" * 100)
    print(f"üß™ FINAL TEST EVALUATION WITH ATTENTION ANALYSIS")
    print("=" * 100)
    
    model.eval()
    test_preds = []
    test_labels = []
    
    with torch.no_grad():
        for batch in test_loader:
            images_dict = {}
            for mag in ADVANCED_TRAINING_CONFIG['magnifications']:
                images_dict[f'mag_{mag}'] = batch['images'][f'mag_{mag}'].to(device, non_blocking=True)
            
            class_labels = batch['class_label'].to(device, non_blocking=True)
            
            class_logits, _ = model(images_dict)
            preds = torch.argmax(class_logits, dim=1)
            
            test_preds.append(preds.cpu())
            test_labels.append(class_labels.cpu())
    
    test_preds = torch.cat(test_preds).numpy()
    test_labels = torch.cat(test_labels).numpy()
    
    # Final metrics
    test_accuracy = accuracy_score(test_labels, test_preds)
    test_balanced_acc = balanced_accuracy_score(test_labels, test_preds)
    test_precision = precision_score(test_labels, test_preds, average='binary')
    test_recall = recall_score(test_labels, test_preds, average='binary')
    test_f1 = f1_score(test_labels, test_preds, average='binary')
    
    print(f"üéØ FINAL ADVANCED ATTENTION RESULTS:")
    print(f"   Accuracy: {test_accuracy:.4f}")
    print(f"   Balanced Accuracy: {test_balanced_acc:.4f}")
    print(f"   Precision: {test_precision:.4f}")
    print(f"   Recall: {test_recall:.4f}")
    print(f"   F1-Score: {test_f1:.4f}")
    
    # Final confusion matrix
    test_cm = confusion_matrix(test_labels, test_preds)
    print(f"\nüîç Final Confusion Matrix:")
    print(f"   [[TN={test_cm[0,0]}, FP={test_cm[0,1]}],")
    print(f"    [FN={test_cm[1,0]}, TP={test_cm[1,1]}]]")
    
    # Classification report
    print(f"\nüìã Classification Report:")
    print(classification_report(test_labels, test_preds, target_names=['Benign', 'Malignant']))
    
    # Save results
    results_summary = {
        'model_type': 'AdvancedMultiMagAttentionNet',
        'backbone': ADVANCED_TRAINING_CONFIG['backbone'],
        'attention_mechanisms': 'Hierarchical + Spatial + Channel + Cross-Magnification',
        'parameters': sum(p.numel() for p in model.parameters()),
        'test_patients': len(test_patients),
        'test_accuracy': test_accuracy,
        'test_balanced_accuracy': test_balanced_acc,
        'test_f1': test_f1,
        'magnification_importance': test_mag_importance,
    }
    
    results_df = pd.DataFrame([results_summary])
    results_df.to_csv('advanced_attention_results.csv', index=False)
    
    # Final summary
    print(f"\n" + "=" * 100)
    print(f"üèÜ ADVANCED ATTENTION TRAINING COMPLETE")
    print("=" * 100)
    print(f"‚úÖ State-of-the-art attention mechanisms implemented")
    print(f"‚úÖ Hierarchical magnification attention (40x‚Üí100x‚Üí200x‚Üí400x)")
    print(f"‚úÖ Multi-scale spatial attention with channel attention")
    print(f"‚úÖ Learnable magnification importance weights")
    print(f"‚úÖ Comprehensive attention visualization")
    print(f"")
    print(f"üéØ Final Performance:")
    print(f"   Test Accuracy: {test_accuracy:.1%}")
    print(f"   Balanced Accuracy: {test_balanced_acc:.1%}")
    print(f"   F1-Score: {test_f1:.1%}")
    print(f"")
    print(f"üî¨ Journal Publication Quality:")
    if 0.85 <= test_accuracy <= 0.96:
        print(f"   ‚úÖ Realistic medical AI performance achieved")
        print(f"   ‚úÖ Suitable for top-tier journal submission")
    else:
        print(f"   üìä Performance: {test_accuracy:.1%} - assess model/data")
    print(f"")
    print(f"üìÅ Results saved to: advanced_attention_results.csv")
    print(f"üé® Attention visualizations available via model.get_attention_maps()")
    print("=" * 100)
    
    return results_summary


if __name__ == "__main__":
    results = main()
    print("\nüéâ Advanced attention-guided multi-magnification training completed!")
    print("üèÜ Ready for journal publication with SOTA attention mechanisms!")