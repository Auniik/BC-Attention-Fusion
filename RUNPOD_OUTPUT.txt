Dataset Statistics:
--------------------------------------------------------------------------------
Category                                 Images     Patients
--------------------------------------------------------------------------------
benign_adenosis_100X                     113        4
benign_adenosis_200X                     111        4
benign_adenosis_400X                     106        4
benign_adenosis_40X                      114        4
benign_fibroadenoma_100X                 260        10
benign_fibroadenoma_200X                 264        10
benign_fibroadenoma_400X                 237        10
benign_fibroadenoma_40X                  253        10
benign_phyllodes_tumor_100X              121        3
benign_phyllodes_tumor_200X              108        3
benign_phyllodes_tumor_400X              115        3
benign_phyllodes_tumor_40X               109        3
benign_tubular_adenoma_100X              150        7
benign_tubular_adenoma_200X              140        7
benign_tubular_adenoma_400X              130        7
benign_tubular_adenoma_40X               149        7
malignant_ductal_carcinoma_100X          903        38
malignant_ductal_carcinoma_200X          896        38
malignant_ductal_carcinoma_400X          788        38
malignant_ductal_carcinoma_40X           864        38
malignant_lobular_carcinoma_100X         170        5
malignant_lobular_carcinoma_200X         163        5
malignant_lobular_carcinoma_400X         137        5
malignant_lobular_carcinoma_40X          156        5
malignant_mucinous_carcinoma_100X        222        9
malignant_mucinous_carcinoma_200X        196        9
malignant_mucinous_carcinoma_400X        169        9
malignant_mucinous_carcinoma_40X         205        9
malignant_papillary_carcinoma_100X       142        6
malignant_papillary_carcinoma_200X       135        6
malignant_papillary_carcinoma_400X       138        6
malignant_papillary_carcinoma_40X        145        6

Folds.csv Info:
Shape: (197725, 4)

Columns: ['fold', 'mag', 'grp', 'filename']

First few rows:
   fold  mag    grp                                           filename
0     1  100  train  BreaKHis_v1/histology_slides/breast/benign/SOB...
1     1  100  train  BreaKHis_v1/histology_slides/breast/benign/SOB...
2     1  100  train  BreaKHis_v1/histology_slides/breast/benign/SOB...
3     1  100  train  BreaKHis_v1/histology_slides/breast/benign/SOB...
4     1  100  train  BreaKHis_v1/histology_slides/breast/benign/SOB...

Fold distribution:
fold
1    39545
2    39545
3    39545
4    39545
5    39545
Name: count, dtype: int64

Magnification distribution:
mag
100    52025
200    50325
40     49875
400    45500
Name: count, dtype: int64

Train/Test distribution:
grp
train    158180
test      39545
Name: count, dtype: int64
        fold  mag    grp                                           filename
0          1  100  train  BreaKHis_v1/histology_slides/breast/benign/SOB...
1          1  100  train  BreaKHis_v1/histology_slides/breast/benign/SOB...
2          1  100  train  BreaKHis_v1/histology_slides/breast/benign/SOB...
3          1  100  train  BreaKHis_v1/histology_slides/breast/benign/SOB...
4          1  100  train  BreaKHis_v1/histology_slides/breast/benign/SOB...
...      ...  ...    ...                                                ...
197720     5  400  train  BreaKHis_v1/histology_slides/breast/malignant/...
197721     5  400  train  BreaKHis_v1/histology_slides/breast/malignant/...
197722     5  400  train  BreaKHis_v1/histology_slides/breast/malignant/...
197723     5  400  train  BreaKHis_v1/histology_slides/breast/malignant/...
197724     5  400  train  BreaKHis_v1/histology_slides/breast/malignant/...

[197725 rows x 4 columns]
Dataset structure analysis:
Unique patients: 82

Tumor types distribution:
tumor_class  tumor_type
benign       adenosis               11100
             fibroadenoma           25350
             phyllodes_tumor        11325
             tubular_adenoma        14225
malignant    ductal_carcinoma       86275
             lobular_carcinoma      15650
             mucinous_carcinoma     19800
             papillary_carcinoma    14000
dtype: int64

Patients by number of magnifications available:
magnification
4    82
Name: count, dtype: int64

Images per patient-magnification (sample):
patient_id          magnification
SOB_B_A_14-22549AB  40               725
                    100              750
                    200              800
                    400              750
SOB_B_A_14-22549CD  40               875
                    100              900
                    200              775
                    400              750
SOB_B_A_14-22549G   40               875
                    100              850
                    200              800
                    400              725
SOB_B_A_14-29960CD  40               375
                    100              325
                    200              400
                    400              425
SOB_B_F_14-14134    40               900
                    100              775
                    200              950
                    400              925
dtype: int64

============================================================
FOLD 1 ANALYSIS
============================================================
=== CLASS DISTRIBUTION ANALYSIS ===

Class distribution by split:
tumor_class  benign  malignant
grp
test           3400       5260
train          9000      21885

Class ratio (benign:malignant) in train: 9000:21885

=== TUMOR TYPE DISTRIBUTION ===
tumor_class  tumor_type
benign       adenosis                1570
             fibroadenoma            3495
             phyllodes_tumor         1090
             tubular_adenoma         2845
malignant    ductal_carcinoma       14375
             lobular_carcinoma       1990
             mucinous_carcinoma      3045
             papillary_carcinoma     2475
dtype: int64

=== PATIENT-LEVEL ANALYSIS ===
Unique patients - Benign: 24, Malignant: 58

Images per patient stats:
Mean: 475.2, Std: 179.0

=== MAGNIFICATION BALANCE ===
mag           40    100   200   400
tumor_class
benign       2290  2295  2275  2140
malignant    5580  5750  5630  4925

=== CALCULATED WEIGHTS ===
Class weights: {'malignant': 0.705620287868403, 'benign': 1.7158333333333333}

Tumor type weights (top 5):
  phyllodes_tumor: 3.542
  adenosis: 2.459
  lobular_carcinoma: 1.940
  papillary_carcinoma: 1.560
  tubular_adenoma: 1.357

üìà Dataset Statistics:
   Total patients: 82
   Training samples: 119000
   Test samples: 44475
üîç Mode 'train' filtering:
   Total available patients: 82
   Patients with train samples: 50
   Filtered patients for train: 50
   ‚úÖ No patient overlap between train and test
üîç Mode 'val' filtering:
   Total available patients: 82
   Patients with val samples: 15
   Filtered patients for val: 15
üîç Mode 'test' filtering:
   Total available patients: 82
   Patients with test samples: 17
   Filtered patients for test: 17

üéØ Patient Splits:
   Train: 50 patients
   Validation: 15 patients
   Test: 17 patients
Balanced dataset: 560 benign, 560 malignant
Dataset initialized:
  Mode: train
  Patients: 50
  Samples per epoch: 1120
  Magnifications: ['40', '100', '200', '400']
Dataset initialized:
  Mode: val
  Patients: 15
  Samples per epoch: 180
  Magnifications: ['40', '100', '200', '400']
Dataset initialized:
  Mode: test
  Patients: 17
  Samples per epoch: 204
  Magnifications: ['40', '100', '200', '400']

üìä Dataset Sizes (Increased for Better Training):
   Train: 1120 samples (vs 840)
   Validation: 180 samples
   Test: 204 samples

üß† Initializing Lightweight Attention Model...
üöÄ Initializing LightweightAttentionNet with 4 magnifications
model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21.4M/21.4M [00:00<00:00, 64.2MB/s]
üìä Feature dimensions: 1280 channels
‚úÖ Lightweight model initialized successfully!

üèóÔ∏è  LightweightAttentionNet Architecture Summary
======================================================================
üìä Input: 4 magnifications (40x, 100x, 200x, 400x)
üß† Backbone: EfficientNet-B0 per magnification
üéØ Feature dimension: 1280
‚ö° Total parameters: 31,444,222
üîß Trainable parameters: 31,444,222
üìä Params per training sample: 37433.6

üîç Attention Mechanisms:
   ‚Ä¢ Channel attention (reduction=8)
   ‚Ä¢ Cross-magnification fusion attention
   ‚Ä¢ Learned magnification importance weights

üìã Classification Heads:
   ‚Ä¢ Binary classification: 2 classes
   ‚Ä¢ Tumor type classification: 8 classes
======================================================================

üöÄ Starting Fixed Training...

üöÄ STARTING LIGHTWEIGHT ATTENTION TRAINING
üîß Fixes applied for 29.4% accuracy failure:
   ‚úÖ Class-weighted focal loss for prediction collapse
   ‚úÖ Gradient clipping for training stability
   ‚úÖ Better learning rate schedule
   ‚úÖ Proper patience and early stopping

/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
üéØ Training parameters:
   Class weights: Benign=1.00, Malignant=0.40
   Learning rate: 0.001
   Gradient clipping: 1.0
   Patience: 15

Epoch 1/50
--------------------------------------------------
   Batch 1/70: Loss=0.6268
   Batch 21/70: Loss=0.2554
   Batch 41/70: Loss=0.2099
   Batch 61/70: Loss=0.2111
Train - Loss: 0.2832, Acc: 0.7080
Val - Loss: 0.4303, Acc: 0.6111, Bal_Acc: 0.6553, F1: 0.6331
Per-class Acc: Benign=0.7500, Malignant=0.5606
LR: 1.00e-03
‚úÖ New best model saved! Balanced Acc: 0.6553

Epoch 2/50
--------------------------------------------------
   Batch 1/70: Loss=0.2748
   Batch 21/70: Loss=0.2055
   Batch 41/70: Loss=0.2942
   Batch 61/70: Loss=0.2482
Train - Loss: 0.2112, Acc: 0.6893
Val - Loss: 0.4874, Acc: 0.2667, Bal_Acc: 0.5000, F1: 0.1123
Per-class Acc: Benign=1.0000, Malignant=0.0000
LR: 1.00e-03
‚è≥ No improvement. Patience: 1/15

Epoch 3/50
--------------------------------------------------
   Batch 1/70: Loss=0.1882
   Batch 21/70: Loss=0.2029
   Batch 41/70: Loss=0.1569
   Batch 61/70: Loss=0.1612
Train - Loss: 0.1865, Acc: 0.7607
Val - Loss: 0.3304, Acc: 0.8389, Bal_Acc: 0.8106, F1: 0.8413
Per-class Acc: Benign=0.7500, Malignant=0.8712
LR: 1.00e-03
‚úÖ New best model saved! Balanced Acc: 0.8106

Epoch 4/50
--------------------------------------------------
   Batch 1/70: Loss=0.2269
   Batch 21/70: Loss=0.1636
   Batch 41/70: Loss=0.2195
   Batch 61/70: Loss=0.1939
Train - Loss: 0.1838, Acc: 0.7804
Val - Loss: 0.3897, Acc: 0.8000, Bal_Acc: 0.8636, F1: 0.8115
Per-class Acc: Benign=1.0000, Malignant=0.7273
LR: 1.00e-03
‚úÖ New best model saved! Balanced Acc: 0.8636

Epoch 5/50
--------------------------------------------------
   Batch 1/70: Loss=0.2032
   Batch 21/70: Loss=0.0837
   Batch 41/70: Loss=0.1241
   Batch 61/70: Loss=0.1604
Train - Loss: 0.1372, Acc: 0.8545
Val - Loss: 0.4431, Acc: 0.8000, Bal_Acc: 0.7841, F1: 0.8063
Per-class Acc: Benign=0.7500, Malignant=0.8182
LR: 1.00e-03
‚è≥ No improvement. Patience: 1/15

Epoch 6/50
--------------------------------------------------
   Batch 1/70: Loss=0.0784
   Batch 21/70: Loss=0.1294
   Batch 41/70: Loss=0.2078
   Batch 61/70: Loss=0.1064
Train - Loss: 0.1563, Acc: 0.7625
Val - Loss: 0.4072, Acc: 0.7944, Bal_Acc: 0.8532, F1: 0.8062
Per-class Acc: Benign=0.9792, Malignant=0.7273
LR: 1.00e-03
‚è≥ No improvement. Patience: 2/15

Epoch 7/50
--------------------------------------------------
   Batch 1/70: Loss=0.1656
   Batch 21/70: Loss=0.0774
   Batch 41/70: Loss=0.1077
   Batch 61/70: Loss=0.1698
Train - Loss: 0.1361, Acc: 0.8286
Val - Loss: 0.4070, Acc: 0.8000, Bal_Acc: 0.7841, F1: 0.8063
Per-class Acc: Benign=0.7500, Malignant=0.8182
LR: 1.00e-03
‚è≥ No improvement. Patience: 3/15

Epoch 8/50
--------------------------------------------------
   Batch 1/70: Loss=0.2027
   Batch 21/70: Loss=0.1445
   Batch 41/70: Loss=0.1038
   Batch 61/70: Loss=0.1958
Train - Loss: 0.1181, Acc: 0.8107
Val - Loss: 0.5452, Acc: 0.7444, Bal_Acc: 0.6799, F1: 0.7461
Per-class Acc: Benign=0.5417, Malignant=0.8182
LR: 1.00e-03
‚è≥ No improvement. Patience: 4/15

Epoch 9/50
--------------------------------------------------
   Batch 1/70: Loss=0.0797
   Batch 21/70: Loss=0.0807
   Batch 41/70: Loss=0.1255
   Batch 61/70: Loss=0.1106
Train - Loss: 0.1091, Acc: 0.8625
Val - Loss: 0.5380, Acc: 0.5389, Bal_Acc: 0.6856, F1: 0.5401
Per-class Acc: Benign=1.0000, Malignant=0.3712
LR: 1.00e-03
‚è≥ No improvement. Patience: 5/15

Epoch 10/50
--------------------------------------------------
   Batch 1/70: Loss=0.1152
   Batch 21/70: Loss=0.0731
   Batch 41/70: Loss=0.0470
   Batch 61/70: Loss=0.0611
Train - Loss: 0.1149, Acc: 0.8080
Val - Loss: 0.6186, Acc: 0.8000, Bal_Acc: 0.8636, F1: 0.8115
Per-class Acc: Benign=1.0000, Malignant=0.7273
LR: 5.00e-04
‚è≥ No improvement. Patience: 6/15

Epoch 11/50
--------------------------------------------------
   Batch 1/70: Loss=0.1240
   Batch 21/70: Loss=0.0830
   Batch 41/70: Loss=0.0867
   Batch 61/70: Loss=0.0897
Train - Loss: 0.0893, Acc: 0.8384
Val - Loss: 0.5776, Acc: 0.6667, Bal_Acc: 0.6932, F1: 0.6858
Per-class Acc: Benign=0.7500, Malignant=0.6364
LR: 5.00e-04
‚è≥ No improvement. Patience: 7/15

Epoch 12/50
--------------------------------------------------
   Batch 1/70: Loss=0.1228
   Batch 21/70: Loss=0.0994
   Batch 41/70: Loss=0.0502
   Batch 61/70: Loss=0.0910
Train - Loss: 0.0679, Acc: 0.9098
Val - Loss: 0.5136, Acc: 0.7222, Bal_Acc: 0.7311, F1: 0.7366
Per-class Acc: Benign=0.7500, Malignant=0.7121
LR: 5.00e-04
‚è≥ No improvement. Patience: 8/15

Epoch 13/50
--------------------------------------------------
   Batch 1/70: Loss=0.0341
   Batch 21/70: Loss=0.0912
   Batch 41/70: Loss=0.0179
   Batch 61/70: Loss=0.0181
Train - Loss: 0.0513, Acc: 0.9134
Val - Loss: 0.5774, Acc: 0.8000, Bal_Acc: 0.7841, F1: 0.8063
Per-class Acc: Benign=0.7500, Malignant=0.8182
LR: 5.00e-04
‚è≥ No improvement. Patience: 9/15

Epoch 14/50
--------------------------------------------------
   Batch 1/70: Loss=0.0409
   Batch 21/70: Loss=0.0348
   Batch 41/70: Loss=0.0347
   Batch 61/70: Loss=0.0190
Train - Loss: 0.0579, Acc: 0.9143
Val - Loss: 0.5426, Acc: 0.8667, Bal_Acc: 0.8295, F1: 0.8667
Per-class Acc: Benign=0.7500, Malignant=0.9091
LR: 5.00e-04
‚è≥ No improvement. Patience: 10/15

Epoch 15/50
--------------------------------------------------
   Batch 1/70: Loss=0.0493
   Batch 21/70: Loss=0.0178
   Batch 41/70: Loss=0.0414
   Batch 61/70: Loss=0.0339
Train - Loss: 0.0581, Acc: 0.9089
Val - Loss: 0.5621, Acc: 0.8000, Bal_Acc: 0.7841, F1: 0.8063
Per-class Acc: Benign=0.7500, Malignant=0.8182
LR: 5.00e-04
‚è≥ No improvement. Patience: 11/15

Epoch 16/50
--------------------------------------------------
   Batch 1/70: Loss=0.0619
   Batch 21/70: Loss=0.0239
   Batch 41/70: Loss=0.0238
   Batch 61/70: Loss=0.0170
Train - Loss: 0.0591, Acc: 0.9000
Val - Loss: 0.5109, Acc: 0.8667, Bal_Acc: 0.9091, F1: 0.8733
Per-class Acc: Benign=1.0000, Malignant=0.8182
LR: 5.00e-04
‚úÖ New best model saved! Balanced Acc: 0.9091

Epoch 17/50
--------------------------------------------------
   Batch 1/70: Loss=0.0557
   Batch 21/70: Loss=0.0273
   Batch 41/70: Loss=0.0556
   Batch 61/70: Loss=0.0474
Train - Loss: 0.0510, Acc: 0.9330
Val - Loss: 0.4101, Acc: 0.9333, Bal_Acc: 0.9545, F1: 0.9354
Per-class Acc: Benign=1.0000, Malignant=0.9091
LR: 5.00e-04
‚úÖ New best model saved! Balanced Acc: 0.9545

Epoch 18/50
--------------------------------------------------
   Batch 1/70: Loss=0.0211
   Batch 21/70: Loss=0.0182
   Batch 41/70: Loss=0.0307
   Batch 61/70: Loss=0.0250
Train - Loss: 0.0459, Acc: 0.9304
Val - Loss: 0.5305, Acc: 0.9722, Bal_Acc: 0.9811, F1: 0.9726
Per-class Acc: Benign=1.0000, Malignant=0.9621
LR: 5.00e-04
‚úÖ New best model saved! Balanced Acc: 0.9811

Epoch 19/50
--------------------------------------------------
   Batch 1/70: Loss=0.0228
   Batch 21/70: Loss=0.0251
   Batch 41/70: Loss=0.0431
   Batch 61/70: Loss=0.0270
Train - Loss: 0.0375, Acc: 0.9357
Val - Loss: 0.4795, Acc: 0.8667, Bal_Acc: 0.8295, F1: 0.8667
Per-class Acc: Benign=0.7500, Malignant=0.9091
LR: 5.00e-04
‚è≥ No improvement. Patience: 1/15

Epoch 20/50
--------------------------------------------------
   Batch 1/70: Loss=0.0128
   Batch 21/70: Loss=0.0183
   Batch 41/70: Loss=0.1165
   Batch 61/70: Loss=0.0218
Train - Loss: 0.0406, Acc: 0.9357
Val - Loss: 0.6373, Acc: 0.9333, Bal_Acc: 0.9545, F1: 0.9354
Per-class Acc: Benign=1.0000, Malignant=0.9091
LR: 5.00e-04
‚è≥ No improvement. Patience: 2/15

Epoch 21/50
--------------------------------------------------
   Batch 1/70: Loss=0.0469
   Batch 21/70: Loss=0.0196
   Batch 41/70: Loss=0.0105
   Batch 61/70: Loss=0.1051
Train - Loss: 0.0343, Acc: 0.9580
Val - Loss: 0.6693, Acc: 0.8667, Bal_Acc: 0.8295, F1: 0.8667
Per-class Acc: Benign=0.7500, Malignant=0.9091
LR: 5.00e-04
‚è≥ No improvement. Patience: 3/15

Epoch 22/50
--------------------------------------------------
   Batch 1/70: Loss=0.0027
   Batch 21/70: Loss=0.0116
   Batch 41/70: Loss=0.0142
   Batch 61/70: Loss=0.0584
Train - Loss: 0.0459, Acc: 0.9437
Val - Loss: 0.5520, Acc: 0.8667, Bal_Acc: 0.8295, F1: 0.8667
Per-class Acc: Benign=0.7500, Malignant=0.9091
LR: 5.00e-04
‚è≥ No improvement. Patience: 4/15

Epoch 23/50
--------------------------------------------------
   Batch 1/70: Loss=0.0105
   Batch 21/70: Loss=0.1097
   Batch 41/70: Loss=0.0914
   Batch 61/70: Loss=0.0074
Train - Loss: 0.0485, Acc: 0.9330
Val - Loss: 0.7121, Acc: 0.9056, Bal_Acc: 0.8561, F1: 0.9031
Per-class Acc: Benign=0.7500, Malignant=0.9621
LR: 5.00e-04
‚è≥ No improvement. Patience: 5/15

Epoch 24/50
--------------------------------------------------
   Batch 1/70: Loss=0.0498
   Batch 21/70: Loss=0.0273
   Batch 41/70: Loss=0.0207
   Batch 61/70: Loss=0.0360
Train - Loss: 0.0379, Acc: 0.9554
Val - Loss: 0.5527, Acc: 0.7722, Bal_Acc: 0.8447, F1: 0.7854
Per-class Acc: Benign=1.0000, Malignant=0.6894
LR: 2.50e-04
‚è≥ No improvement. Patience: 6/15

Epoch 25/50
--------------------------------------------------
   Batch 1/70: Loss=0.0108
   Batch 21/70: Loss=0.1460
   Batch 41/70: Loss=0.0260
   Batch 61/70: Loss=0.1008
Train - Loss: 0.0418, Acc: 0.9250
Val - Loss: 0.5270, Acc: 0.7389, Bal_Acc: 0.7491, F1: 0.7522
Per-class Acc: Benign=0.7708, Malignant=0.7273
LR: 2.50e-04
‚è≥ No improvement. Patience: 7/15

Epoch 26/50
--------------------------------------------------
   Batch 1/70: Loss=0.0432
   Batch 21/70: Loss=0.0571
   Batch 41/70: Loss=0.0476
   Batch 61/70: Loss=0.0181
Train - Loss: 0.0245, Acc: 0.9607
Val - Loss: 0.5704, Acc: 0.8667, Bal_Acc: 0.9091, F1: 0.8733
Per-class Acc: Benign=1.0000, Malignant=0.8182
LR: 2.50e-04
‚è≥ No improvement. Patience: 8/15

Epoch 27/50
--------------------------------------------------
   Batch 1/70: Loss=0.0272
   Batch 21/70: Loss=0.0748
   Batch 41/70: Loss=0.0051
   Batch 61/70: Loss=0.0357
Train - Loss: 0.0246, Acc: 0.9634
Val - Loss: 0.6094, Acc: 0.8667, Bal_Acc: 0.9091, F1: 0.8733
Per-class Acc: Benign=1.0000, Malignant=0.8182
LR: 2.50e-04
‚è≥ No improvement. Patience: 9/15

Epoch 28/50
--------------------------------------------------
   Batch 1/70: Loss=0.0032
   Batch 21/70: Loss=0.0067
   Batch 41/70: Loss=0.0206
   Batch 61/70: Loss=0.0306
Train - Loss: 0.0256, Acc: 0.9679
Val - Loss: 0.5751, Acc: 0.8667, Bal_Acc: 0.8295, F1: 0.8667
Per-class Acc: Benign=0.7500, Malignant=0.9091
LR: 2.50e-04
‚è≥ No improvement. Patience: 10/15

Epoch 29/50
--------------------------------------------------
   Batch 1/70: Loss=0.0109
   Batch 21/70: Loss=0.0189
   Batch 41/70: Loss=0.0532
   Batch 61/70: Loss=0.0106
Train - Loss: 0.0230, Acc: 0.9732
Val - Loss: 0.5235, Acc: 0.9111, Bal_Acc: 0.9394, F1: 0.9146
Per-class Acc: Benign=1.0000, Malignant=0.8788
LR: 2.50e-04
‚è≥ No improvement. Patience: 11/15

Epoch 30/50
--------------------------------------------------
   Batch 1/70: Loss=0.0039
   Batch 21/70: Loss=0.0018
   Batch 41/70: Loss=0.0064
   Batch 61/70: Loss=0.0209
Train - Loss: 0.0190, Acc: 0.9714
Val - Loss: 0.5115, Acc: 0.9333, Bal_Acc: 0.9545, F1: 0.9354
Per-class Acc: Benign=1.0000, Malignant=0.9091
LR: 1.25e-04
‚è≥ No improvement. Patience: 12/15

Epoch 31/50
--------------------------------------------------
   Batch 1/70: Loss=0.0325
   Batch 21/70: Loss=0.0071
   Batch 41/70: Loss=0.0030
   Batch 61/70: Loss=0.0046
Train - Loss: 0.0195, Acc: 0.9705
Val - Loss: 0.5453, Acc: 0.9333, Bal_Acc: 0.9545, F1: 0.9354
Per-class Acc: Benign=1.0000, Malignant=0.9091
LR: 1.25e-04
‚è≥ No improvement. Patience: 13/15

Epoch 32/50
--------------------------------------------------
   Batch 1/70: Loss=0.0063
   Batch 21/70: Loss=0.0154
   Batch 41/70: Loss=0.0041
   Batch 61/70: Loss=0.0026
Train - Loss: 0.0173, Acc: 0.9812
Val - Loss: 0.5707, Acc: 0.9333, Bal_Acc: 0.9545, F1: 0.9354
Per-class Acc: Benign=1.0000, Malignant=0.9091
LR: 1.25e-04
‚è≥ No improvement. Patience: 14/15

Epoch 33/50
--------------------------------------------------
   Batch 1/70: Loss=0.0536
   Batch 21/70: Loss=0.0045
   Batch 41/70: Loss=0.0092
   Batch 61/70: Loss=0.0313
Train - Loss: 0.0202, Acc: 0.9857
Val - Loss: 0.5889, Acc: 0.9333, Bal_Acc: 0.9545, F1: 0.9354
Per-class Acc: Benign=1.0000, Malignant=0.9091
LR: 1.25e-04
‚è≥ No improvement. Patience: 15/15
‚èπÔ∏è Early stopping triggered after 33 epochs

üéØ Training completed!
   Best accuracy: 0.9722
   Best balanced accuracy: 0.9811

üìÇ Loading best model for final evaluation...
/workspace/BC-Attention-Fusion/main_lightweight_attention.py:445: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load('output/lightweight_model_best.pth', map_location=device)
   Best validation accuracy: 0.9722
   Best balanced accuracy: 0.9811

====================================================================================================
üß™ FINAL TEST EVALUATION
====================================================================================================
üéØ FINAL RESULTS (Fixed Model):
   Test Accuracy: 0.8480 (vs 0.294 failed)
   Balanced Accuracy: 0.8340 (vs 0.500 failed)
   Precision: 0.9124 (vs 0.000 failed)
   Recall: 0.8681 (vs 0.000 failed)
   F1-Score: 0.8897 (vs 0.000 failed)
   Benign Accuracy: 0.8000
   Malignant Accuracy: 0.8681

üîç Final Confusion Matrix (vs [[40,0],[96,0]] failed):
   [[TN=48, FP=12],
    [FN=19, TP=125]]

üìã Classification Report:
              precision    recall  f1-score   support

      Benign       0.72      0.80      0.76        60
   Malignant       0.91      0.87      0.89       144

    accuracy                           0.85       204
   macro avg       0.81      0.83      0.82       204
weighted avg       0.85      0.85      0.85       204


====================================================================================================
üèÜ TRAINING RESULTS ANALYSIS
====================================================================================================
‚úÖ SUCCESS CRITERIA:
   Accuracy > 70%: ‚úÖ PASSED
   Balanced Accuracy > 70%: ‚úÖ PASSED
   Both classes predicted: ‚úÖ PASSED
   Malignant recall > 50%: ‚úÖ PASSED
   No prediction collapse: ‚úÖ PASSED

üéâ SUCCESS! Model training fixed - suitable for journal publication
üìà Performance improvement: 84.8% vs 29.4% failed model

üìÅ Results saved to: lightweight_attention_results.csv
====================================================================================================

üéâ Lightweight attention training completed with all fixes applied!